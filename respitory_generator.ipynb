{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load various imports\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Conv1D\n",
    "from tensorflow.keras.layers import LeakyReLU,  BatchNormalization, Activation, Flatten, MaxPooling1D, Input\n",
    "from sincnet_tensorflow import SincConv1D, LayerNorm\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):\n",
    "    noise_value = 0.015 * np.random.uniform() * np.amax(data)   # 1.5% with variation from 0-1 with emphasis on 0.5 of the maximum value \n",
    "    data = data + noise_value * np.random.normal(size=data.shape[0])   # Add noise to the data\n",
    "    return data\n",
    "\n",
    "def stretch_process(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate=0.8) #elongate data\n",
    "\n",
    "def pitch_process(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor) # Shifts the pitch of the audio data by a specified number of semitones.\n",
    "#this shift is very subtle, 0.7 of a semitone is less than 1/12 of a octave. barely any change jjust a little \n",
    "\n",
    "def extract_process(data, sample_rate, debug=False):\n",
    "    output_result = np.array([])\n",
    "    #amt of times  signal crosses zero (measuring silence) \n",
    "    mean_zero = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0) \n",
    "    if debug: print('mean_zero shape',mean_zero.shape) \n",
    "    #you can horizontally add elements that have the same amount of rows without having to flatten the array\n",
    "\n",
    "    output_result = np.hstack((output_result, mean_zero)) \n",
    "    #positive of all values + short time ft (measuring loudness)  + zero crossing rate (measuring silence) \n",
    "    stft_out = np.abs(librosa.stft(data))\n",
    "    if debug: print('stft_out shape', stft_out.shape) \n",
    "\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft_out, sr=sample_rate).T, axis=0)\n",
    "    output_result = np.hstack((output_result, chroma_stft))\n",
    "    if debug: print('chroma_stft shape',chroma_stft.shape) \n",
    "    \n",
    "    mfcc_out = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "    output_result = np.hstack((output_result, mfcc_out))\n",
    "    if debug: print('mfcc_out shape',mfcc_out.shape) \n",
    "\n",
    "    root_mean_out = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    output_result = np.hstack((output_result, root_mean_out)) \n",
    "    if debug:  print('root_mean_out shape',root_mean_out.shape) \n",
    "\n",
    "    mel_spectogram = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    output_result = np.hstack((output_result, mel_spectogram))\n",
    "    if debug:  print('mel_spectogram shape',mel_spectogram.shape) \n",
    "\n",
    "\n",
    "    return output_result\n",
    "\n",
    "def extract_features(file_name, debug=False, extract=True, length = 427770):\n",
    "    try:\n",
    "        # Load the original audio file\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=42, offset=0.6)\n",
    "        if debug: print(sample_rate)\n",
    "        assert  sample_rate == 22050, \"Sample rate is not 22050\" \n",
    "        \n",
    "        l = audio.shape[0]\n",
    "        \n",
    "        if(audio.shape[0]< length):\n",
    "            audio = np.pad(audio, (0, length - audio.shape[0]))\n",
    "        else:\n",
    "            audio = audio[:length]\n",
    "        \n",
    "\n",
    "        if(extract):  # If extract is True, augment the audio file\n",
    "\n",
    "            # Extract features from the original audio data\n",
    "            extracted_features = extract_process(audio, sample_rate, debug=False)\n",
    "            result = np.array(extracted_features)\n",
    "            \n",
    "        else: \n",
    "            result = np.array(audio)\n",
    "            \n",
    "        if debug: print(\"result shape before nouse\",result.shape)\n",
    "            \n",
    "\n",
    "        # Add noise and extract features\n",
    "        noise_out = add_noise(audio)\n",
    "        if(extract): \n",
    "            \n",
    "            output_2 = extract_process(noise_out, sample_rate)\n",
    "            result = np.vstack((result, output_2))\n",
    "        else: \n",
    "            result =  np.vstack((result, noise_out))\n",
    "            \n",
    "        if debug: print(\"result shape before stretch\",result.shape, noise_out.shape) \n",
    "\n",
    "        # Time-stretch and then pitch-shift before extracting features\n",
    "        new_out = stretch_process(audio,0.8)\n",
    "        new_out2 = new_out[0:audio.shape[0]]\n",
    "        stretch_pitch = pitch_process(new_out2, sample_rate,pitch_factor=0.7)\n",
    "        if debug: print(\"before final:\",result.shape,  stretch_pitch.shape) \n",
    "        if(extract): \n",
    "            output_3 = extract_process(stretch_pitch, sample_rate)\n",
    "            result = np.vstack((result, output_3))\n",
    "        else: \n",
    "            result = np.vstack((result, stretch_pitch))\n",
    "            \n",
    "        if debug: print(\"final:\",result.shape,  stretch_pitch.shape) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        print(\"Error Details:\", e)\n",
    "        return None\n",
    "\n",
    "    return result, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n",
      "[['Bronchiectasis' 'Bronchiolitis' 'COPD' 'Healthy' 'Pneumonia' 'URTI']\n",
      " ['16' '13' '793' '35' '37' '23']]\n",
      "917 917\n"
     ]
    }
   ],
   "source": [
    "mypath = 'C:/Users/Agraw/Downloads/archive/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files'\n",
    "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))] \n",
    "print(len(filenames))\n",
    "\n",
    "p_id_in_file = [] # patient IDs corresponding to each file\n",
    "for name in filenames:\n",
    "    p_id_in_file.append(int(name[:3]))\n",
    "\n",
    "p_id_in_file = np.array(p_id_in_file)\n",
    "\n",
    "filepaths = [join(mypath, f) for f in filenames] # full paths of files\n",
    "p_diag=pd.read_csv('C:/Users/Agraw/Downloads/archive/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv',header=None)\n",
    "labels = np.array([p_diag[p_diag[0] == x][1].values[0] for x in p_id_in_file]) # labels for audio files\n",
    "\n",
    "# delete the very rare diseases, rewrite into same array for space? \n",
    "new_filepaths = np.delete(filepaths, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
    "new_labels = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
    "assert len(new_filepaths) == len(new_labels)\n",
    "\n",
    "# print class counts\n",
    "unique_elements, counts_elements = np.unique(new_labels, return_counts=True) \n",
    "#amt of values that have same value,  i.e. same class \n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "print(len(new_filepaths), len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "  def __init__(self, list_IDs, labels, batch_size=32, shuffle=True, debug = False, extract_ = True):\n",
    "    'Initialization'\n",
    "    self.batch_size = batch_size\n",
    "    self.labels = labels\n",
    "    self.list_IDs = list_IDs\n",
    "    self.shuffle = shuffle\n",
    "    self.indexes = np.arange(len(self.list_IDs))\n",
    "    self.debug = debug \n",
    "    self.extract_ =  extract_\n",
    "    self.on_epoch_end()\n",
    "    \n",
    "      \n",
    "  def  __len__(self):\n",
    "    return int(np.floor(len(self.list_IDs) / self.batch_size)) #all files in epoch 1 batch at a time\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size] #[0,31] [32,63] ..\n",
    "    list_IDs_temp = [self.list_IDs[k] for k in indexes] \n",
    "    label_temp = [self.labels[k] for k in indexes]\n",
    "    #does this work... \n",
    "    #list_IDs_temp = new_filepaths \n",
    "      \n",
    "    X, y = self.__data_generation(list_IDs_temp, label_temp)\n",
    "    return X, y\n",
    "\n",
    "  def __data_generation(self, filepath, label):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "      data, l = extract_features(filepath[i], debug=False, extract=self.extract_)\n",
    "      X.append(data)\n",
    "      y.append(label[i]) \n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    \n",
    "    if self.debug: print(X.shape, y.shape)\n",
    "    X = X.reshape(-1, X.shape[2]) # flatten (batch_size, 3, <featrue_size>) -> (batch*3,  <feature_size>)\n",
    "\n",
    "    y = np.repeat(label, 3, axis=0) # augment chagne this code to batch*3,6 not 54 \n",
    "    if self.debug: print(X.shape,  y.shape)\n",
    "\n",
    "    X = np.expand_dims(X,axis=2) #readable format for model\n",
    "    if self.debug: print(X.shape,  y.shape)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() #[0,0,0,0,0,1] []\n",
    "new_labels_1hot = le.fit_transform(new_labels) \n",
    "new_labels_1hot = to_categorical(new_labels_1hot) #using onehot encoding to convert labels into 1hot format [0,0,0,0,0,1]  \n",
    "\n",
    "train_fp, test_fp, label_train, label_test = train_test_split(new_filepaths, new_labels_1hot, stratify=new_labels_1hot, #doesnt work with features \n",
    "                                                     test_size=0.2, random_state = 42)\n",
    "temp = label_train\n",
    "temp2 = train_fp\n",
    "label_train = label_train[:int(len(label_train)*.8)]\n",
    "label_val = temp[int(len(temp)*.8):]\n",
    "train_fp = train_fp[:int(len(train_fp)*.8)]\n",
    "train_val = temp2[int(len(temp2)*.8):]\n",
    "\n",
    "# for d in unique_elements:\n",
    "#     files_trained = train_fp[np.argwhere(label_train ==d)]\n",
    "#     files_tested = test_fp[np.argwhere(label_test ==d)]\n",
    "\n",
    "#     print(len(files_trained), len(files_tested))\n",
    "    \n",
    "batch_size = 32\n",
    "training_generator =  DataGenerator(train_fp, label_train, batch_size=batch_size, shuffle=True, extract_=True, debug = False)\n",
    "testing_generator =  DataGenerator(test_fp, label_test, batch_size=batch_size, shuffle=False, extract_=True, debug = False)\n",
    "validation_generator = DataGenerator(train_val, label_val, batch_size=batch_size, shuffle=True, extract_=True, debug = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "586\n",
      "(96, 182, 1) (96, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n",
      "(96, 182, 1) (96, 6)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_fp\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()): \n\u001b[1;32m----> 6\u001b[0m         x,y \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      8\u001b[0m     train_fp\u001b[38;5;241m.\u001b[39mon_epoch_end()   \u001b[38;5;66;03m# Call on_epoch_end to reset the index\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     21\u001b[0m label_temp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m indexes]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#does this work... \u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#list_IDs_temp = new_filepaths \u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_IDs_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36mDataGenerator.__data_generation\u001b[1;34m(self, filepath, label)\u001b[0m\n\u001b[0;32m     30\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m---> 33\u001b[0m   data, l \u001b[38;5;241m=\u001b[39m extract_features(\u001b[43mfilepath\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extract\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_)\n\u001b[0;32m     34\u001b[0m   X\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     35\u001b[0m   y\u001b[38;5;241m.\u001b[39mappend(label[i]) \n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "assert True \n",
    "print(training_generator.__len__())\n",
    "print(train_fp.__len__())\n",
    "for k in range(2):\n",
    "    for i in range(train_fp.__len__()): \n",
    "        x,y = training_generator.__getitem__(i)\n",
    "        print(x.shape, y.shape)\n",
    "    train_fp.on_epoch_end()   # Call on_epoch_end to reset the index\n",
    "\n",
    "    \n",
    "#training_generator.__getitem__(0)\n",
    "# x 3 -> 3 features -> 3,3, <> \n",
    "# y 3, <>\n",
    "# y 3,3,<> = 3,3,6 => 54... 9, 6\n",
    "# (9, 427770) (54,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tmp_x, tmp_y \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#(32,182,1) (32, 6)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmp_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     21\u001b[0m label_temp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m indexes]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#does this work... \u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#list_IDs_temp = new_filepaths \u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_IDs_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36mDataGenerator.__data_generation\u001b[1;34m(self, filepath, label)\u001b[0m\n\u001b[0;32m     30\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m---> 33\u001b[0m   data, l \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m   X\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     35\u001b[0m   y\u001b[38;5;241m.\u001b[39mappend(label[i]) \n",
      "Cell \u001b[1;32mIn[6], line 85\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_name, debug, extract, length)\u001b[0m\n\u001b[0;32m     83\u001b[0m new_out \u001b[38;5;241m=\u001b[39m stretch_process(audio,\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m     84\u001b[0m new_out2 \u001b[38;5;241m=\u001b[39m new_out[\u001b[38;5;241m0\u001b[39m:audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m---> 85\u001b[0m stretch_pitch \u001b[38;5;241m=\u001b[39m \u001b[43mpitch_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_out2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpitch_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore final:\u001b[39m\u001b[38;5;124m\"\u001b[39m,result\u001b[38;5;241m.\u001b[39mshape,  stretch_pitch\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(extract): \n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mpitch_process\u001b[1;34m(data, sampling_rate, pitch_factor)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpitch_process\u001b[39m(data, sampling_rate, pitch_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpitch_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpitch_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\librosa\\effects.py:482\u001b[0m, in \u001b[0;36mpitch_shift\u001b[1;34m(y, sr, n_steps, bins_per_octave, res_type, scale, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(n_steps) \u001b[38;5;241m/\u001b[39m bins_per_octave)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Stretch in time, then resample\u001b[39;00m\n\u001b[0;32m    481\u001b[0m y_shift \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mresample(\n\u001b[1;32m--> 482\u001b[0m     time_stretch(y, rate\u001b[38;5;241m=\u001b[39mrate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    483\u001b[0m     orig_sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(sr) \u001b[38;5;241m/\u001b[39m rate,\n\u001b[0;32m    484\u001b[0m     target_sr\u001b[38;5;241m=\u001b[39msr,\n\u001b[0;32m    485\u001b[0m     res_type\u001b[38;5;241m=\u001b[39mres_type,\n\u001b[0;32m    486\u001b[0m     scale\u001b[38;5;241m=\u001b[39mscale,\n\u001b[0;32m    487\u001b[0m )\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Crop to the same dimension as the input\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mfix_length(y_shift, size\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\librosa\\effects.py:398\u001b[0m, in \u001b[0;36mtime_stretch\u001b[1;34m(y, rate, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m len_stretch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m rate))\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# Invert the STFT\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m y_stretch \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mistft(stft_stretch, dtype\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mdtype, length\u001b[38;5;241m=\u001b[39mlen_stretch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_stretch\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tmp_x, tmp_y = training_generator.__getitem__(0)\n",
    "#(32,182,1) (32, 6)\n",
    "print(tmp_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "sincmodel = False\n",
    "\n",
    "num_labels = len(np.unique(new_labels))  # Determine the number of unique classes\n",
    "\n",
    "'''\n",
    "right now doing feature extract, want to use only raw data and make new model \n",
    "'''\n",
    "if(sincmodel):\n",
    "    sinc_layer = SincConv1D(N_filt=64, Filt_dim=129, fs=22050, stride=16, padding=\"SAME\")\n",
    "    inputs = Input((427770, 1))\n",
    "\n",
    "    x = sinc_layer(inputs)\n",
    "    x = LayerNorm()(x)\n",
    "\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "\n",
    "    x = Conv1D(2, 3, strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(2, 3, strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(2, 3, strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(2, 3, strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization(momentum=0.05, epsilon=1e-5)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization(momentum=0.05, epsilon=1e-5)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    prediction = Dense(num_labels, activation='softmax')(x)\n",
    "    model = tensorflow.keras.models.Model(inputs=inputs, outputs=prediction)\n",
    "\n",
    "    model.summary()\n",
    "else:\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(tmp_x.shape[1], 1)),\n",
    "        layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'),\n",
    "        layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'),\n",
    "        layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'),\n",
    "        layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'),\n",
    "        layers.Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(units=num_labels, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "early_stop = tensorflow.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=100,mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 628s 35s/step - loss: 3.2838 - accuracy: 0.6730 - val_loss: 0.9067 - val_accuracy: 0.8672\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 556s 31s/step - loss: 1.0505 - accuracy: 0.7731 - val_loss: 0.7658 - val_accuracy: 0.8464\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 540s 30s/step - loss: 0.7817 - accuracy: 0.8079 - val_loss: 0.5940 - val_accuracy: 0.8516\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 534s 30s/step - loss: 0.6911 - accuracy: 0.8345 - val_loss: 0.5023 - val_accuracy: 0.8672\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 530s 30s/step - loss: 0.7027 - accuracy: 0.8293 - val_loss: 0.6299 - val_accuracy: 0.8516\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 532s 30s/step - loss: 0.6903 - accuracy: 0.8385 - val_loss: 0.5836 - val_accuracy: 0.8516\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 535s 30s/step - loss: 0.6750 - accuracy: 0.8455 - val_loss: 0.5557 - val_accuracy: 0.8594\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 534s 30s/step - loss: 0.6247 - accuracy: 0.8478 - val_loss: 0.5728 - val_accuracy: 0.8438\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 532s 30s/step - loss: 0.5838 - accuracy: 0.8582 - val_loss: 0.4895 - val_accuracy: 0.8594\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 533s 30s/step - loss: 0.5920 - accuracy: 0.8600 - val_loss: 0.4529 - val_accuracy: 0.8672\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 538s 30s/step - loss: 0.5525 - accuracy: 0.8675 - val_loss: 0.4618 - val_accuracy: 0.8594\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 531s 30s/step - loss: 0.5642 - accuracy: 0.8628 - val_loss: 0.5180 - val_accuracy: 0.8516\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 534s 30s/step - loss: 0.5759 - accuracy: 0.8640 - val_loss: 0.4349 - val_accuracy: 0.8828\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 532s 30s/step - loss: 0.5341 - accuracy: 0.8663 - val_loss: 0.5235 - val_accuracy: 0.8516\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 530s 30s/step - loss: 0.5401 - accuracy: 0.8669 - val_loss: 0.4105 - val_accuracy: 0.8828\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 533s 30s/step - loss: 0.5254 - accuracy: 0.8681 - val_loss: 0.4538 - val_accuracy: 0.8672\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 531s 30s/step - loss: 0.5159 - accuracy: 0.8611 - val_loss: 0.4267 - val_accuracy: 0.8750\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 534s 30s/step - loss: 0.5101 - accuracy: 0.8657 - val_loss: 0.4602 - val_accuracy: 0.8594\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 536s 30s/step - loss: 0.5065 - accuracy: 0.8663 - val_loss: 0.4963 - val_accuracy: 0.8594\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 547s 31s/step - loss: 0.5500 - accuracy: 0.8594 - val_loss: 0.5192 - val_accuracy: 0.8438\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.8646 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evConv1D_Model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# evConv1D_Model = model.fit_generator(generator=training_generator, validation_data=testing_generator, batch_size=64, epochs=200,)   \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Agraw\\miniconda3\\envs\\flow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evConv1D_Model = model.fit(training_generator, epochs=200, validation_data=validation_generator, callbacks=[early_stop])\n",
    "# evConv1D_Model = model.fit_generator(generator=training_generator, validation_data=testing_generator, batch_size=64, epochs=200,)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Results = model.evaluate(testing_generator)#check\n",
    "print(\"LOSS:  \" + \"%.4f\" % Model_Results[0])\n",
    "print(\"ACCURACY:  \" + \"%.4f\" % Model_Results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(evConv1D_Model.history['loss'], label='Training Loss')\n",
    "plt.plot(evConv1D_Model.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(evConv1D_Model.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(evConv1D_Model.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions\n",
    "import sklearn\n",
    "y_pred = Model.predict(testing_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generating the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Displaying the confusion matrix\n",
    "cm_display = ConfusionMatrixDisplay(confusion_mtx, display_labels=le.classes_).plot()\n",
    "print(sklearn.metrics.f1_score(y_true, y_pred_classes, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
